{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:  Assigning Class Labels. For this assignment, instead of classifying the language of the text, we are going to predict whether the text contains a concrete word or not. Create a new column named ‘CONCRETE’ in your dataframe. The values for this column should be: LOW: if value in MEAN column is <=4; HIGH: if value in MEAN column >4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>POS</th>\n",
       "      <th>INDEX</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>CONCRETE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>achievement</td>\n",
       "      <td>N</td>\n",
       "      <td>3</td>\n",
       "      <td>Bring up academic achievements , awards , and ...</td>\n",
       "      <td>3.06</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>achievement</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>Please list people you have helped , your pers...</td>\n",
       "      <td>3.03</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>activate</td>\n",
       "      <td>V</td>\n",
       "      <td>1</td>\n",
       "      <td>Add activated carbon straight to your vodka .</td>\n",
       "      <td>3.83</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>activate</td>\n",
       "      <td>V</td>\n",
       "      <td>15</td>\n",
       "      <td>Place sensors around your garden , and when a ...</td>\n",
       "      <td>5.51</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adventure</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>Look for a partner that shares your level of a...</td>\n",
       "      <td>2.03</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>verità</td>\n",
       "      <td>N</td>\n",
       "      <td>8</td>\n",
       "      <td>In un modo o nell' altro , la verità viene sem...</td>\n",
       "      <td>2.53</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>viaggio</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>Organizza dei viaggi nel fine settimana quando...</td>\n",
       "      <td>5.03</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>viaggio</td>\n",
       "      <td>N</td>\n",
       "      <td>6</td>\n",
       "      <td>Pesa le tue valigie prima del viaggio per evit...</td>\n",
       "      <td>4.84</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>vista</td>\n",
       "      <td>N</td>\n",
       "      <td>6</td>\n",
       "      <td>è molto importante non perdere di vista la pro...</td>\n",
       "      <td>2.22</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>vista</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>i conigli hanno un ottimo udito e un' ottima v...</td>\n",
       "      <td>5.13</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TARGET POS  INDEX                                               TEXT  \\\n",
       "0   achievement   N      3  Bring up academic achievements , awards , and ...   \n",
       "1   achievement   N      9  Please list people you have helped , your pers...   \n",
       "2      activate   V      1     Add activated carbon straight to your vodka .    \n",
       "3      activate   V     15  Place sensors around your garden , and when a ...   \n",
       "4     adventure   N      9  Look for a partner that shares your level of a...   \n",
       "..          ...  ..    ...                                                ...   \n",
       "95       verità   N      8  In un modo o nell' altro , la verità viene sem...   \n",
       "96      viaggio   N      2  Organizza dei viaggi nel fine settimana quando...   \n",
       "97      viaggio   N      6  Pesa le tue valigie prima del viaggio per evit...   \n",
       "98        vista   N      6  è molto importante non perdere di vista la pro...   \n",
       "99        vista   N      9  i conigli hanno un ottimo udito e un' ottima v...   \n",
       "\n",
       "    MEAN CONCRETE  \n",
       "0   3.06      LOW  \n",
       "1   3.03      LOW  \n",
       "2   3.83      LOW  \n",
       "3   5.51     HIGH  \n",
       "4   2.03      LOW  \n",
       "..   ...      ...  \n",
       "95  2.53      LOW  \n",
       "96  5.03     HIGH  \n",
       "97  4.84     HIGH  \n",
       "98  2.22      LOW  \n",
       "99  5.13     HIGH  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = pd.read_csv(\"CONcreTEXT_trial_EN.tsv\", sep='\\t')\n",
    "italian = pd.read_csv(\"CONcreTEXT_trial_IT.tsv\", sep='\\t')\n",
    "combined = pd.concat([english, italian])\n",
    "combined[\"CONCRETE\"] = np.where(combined['MEAN'] <= 4, 'LOW',  'HIGH')\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:  Train Test split. You should use the sklearn library train_test_split function to create an 80% training set and 20% testing set. No need to create a validation/dev set for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train, combined_test = train_test_split(combined, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Majority Class Baseline. We will create a majority class baseline to evaluate our initial model performance – which is the simplest baseline. The label for each instance in the test data should simply be the majority class found in training data. You should report P, R and F-score and accuracy of this majority baseline model on the test set . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.62      1.00      0.77        25\n",
      "         LOW       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.31      0.50      0.38        40\n",
      "weighted avg       0.39      0.62      0.48        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = ['HIGH'] * len(combined_test)\n",
    "print(metrics.classification_report(combined_test['CONCRETE'],predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Target Length Baseline. We will create another baseline to evaluate our model performance – which takes into account length of the word (in characters) in the ‘TARGET’ column. For this question, let us assume that all words with length >= 5 characters can be classified as HIGH CONCRETE (This assumption may not actually be true, but it is an assumption we are making for the purposes of creating a baseline). The label for each instance in the test data should simply be the HIGH if the word in the ‘TARGET’ column has length >=5 characters and LOW otherwise. You should report P, R and F-score and accuracy of this target length baseline model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.64      0.72      0.68        25\n",
      "         LOW       0.42      0.33      0.37        15\n",
      "\n",
      "    accuracy                           0.57        40\n",
      "   macro avg       0.53      0.53      0.52        40\n",
      "weighted avg       0.56      0.57      0.56        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_train_target=np.where((combined_train['TARGET']).str.len() >=5, 'HIGH','LOW')\n",
    "combined_test_target=np.where((combined_test['TARGET']).str.len() >=5, 'HIGH','LOW')\n",
    "print(metrics.classification_report(combined_test['CONCRETE'],combined_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Naive Bayes Classifier. You can follow the same steps as the previous assignment and use the built-in Naive Bayes model from sklearn to train a classifier for predicting whether the text is HIGH concrete or LOW concrete (instead of what is the language of the text, which was the classification task for the previous assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(combined_train[\"TEXT\"])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, combined_train[\"CONCRETE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = count_vect.transform(combined_test[\"TARGET\"])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.62      1.00      0.77        25\n",
      "         LOW       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.31      0.50      0.38        40\n",
      "weighted avg       0.39      0.62      0.48        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(combined_test['CONCRETE'],predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: Comparing Performance. How does the Naive Bayes classifier perform, when compared against the performance of the two baselines developed in Questions 3 and 4? What are your observations? Write at least 50 words of your own original thoughts describing what you observe when comparing performance of these three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier performs better than the other two baselines developed above because all the Recall, precision and f1-score and support values are higher when compared to the other classifiers. This is because Naive Bayes Classifier is classifying without modifying the criteria used in label categorization. When using Majoring Class Basline we are choosing the label which has high or low values, and considering only high concrete value. When considering target length baseline model we are choosing the label based on the length of the target, which results in the change in the result. THus the precision, recall, f1-score and support values are way lot different for Naive Bayes classifier than the other two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Experiment with at least 3 other thresholds of target length from Question 4. You should try setting various thresholds of target length to classify them as HIGH CONCRETE or LOW CONCRETE. For example, all words < 5 characters in length can be classified as low concrete. You should experiment with at least 3 different thresholds and document your reasons why you chose these thresholds. Report P, R and F-score and accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold 1: Considering length as 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.50      0.36      0.42        25\n",
      "         LOW       0.27      0.40      0.32        15\n",
      "\n",
      "    accuracy                           0.38        40\n",
      "   macro avg       0.39      0.38      0.37        40\n",
      "weighted avg       0.41      0.38      0.38        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_train_target=np.where((combined_train['TARGET']).str.len() >=7, 'HIGH','LOW')\n",
    "combined_test_target=np.where((combined_test['TARGET']).str.len() >=7, 'HIGH','LOW')\n",
    "print(metrics.classification_report(combined_test['CONCRETE'],combined_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold 2: Considering length as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.62      1.00      0.77        25\n",
      "         LOW       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.31      0.50      0.38        40\n",
      "weighted avg       0.39      0.62      0.48        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_train_target=np.where((combined_train['TARGET']).str.len() >=2, 'HIGH','LOW')\n",
    "combined_test_target=np.where((combined_test['TARGET']).str.len() >=2, 'HIGH','LOW')\n",
    "print(metrics.classification_report(combined_test['CONCRETE'],combined_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold 3: Considering length as 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HIGH       0.00      0.00      0.00        25\n",
      "         LOW       0.38      1.00      0.55        15\n",
      "\n",
      "    accuracy                           0.38        40\n",
      "   macro avg       0.19      0.50      0.27        40\n",
      "weighted avg       0.14      0.38      0.20        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_train_target=np.where((combined_train['TARGET']).str.len() >=15, 'HIGH','LOW')\n",
    "combined_test_target=np.where((combined_test['TARGET']).str.len() >=15, 'HIGH','LOW')\n",
    "print(metrics.classification_report(combined_test['CONCRETE'],combined_test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reason for choosing these thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have choosen these thresholds to check now the precision, recall, f1-score and support values are appearing for a medium value, a low value and a high value and distinguishing the values for the given thresholds. WHile considering the low aand high threshold, it can be seen that the P, R and f1-score are 0 or low and high value, but we can observe that the f1-score accuracy is almost same for medium and high threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
